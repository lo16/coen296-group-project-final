# MusicGAN
## Requirements: 
* **Tensorflow r1.0.1**
* Python 2.7
* CUDA 7.5+ (For GPU)

## Introduction
Apply SequenceGAN on music generation application.
This version contains a small sample input file at ./Data/raw/TrainableMidi2.txt
It should run step 1~3. It's too small and will almost definitely result in overfit.
Use tool provided with step 0 to create large dataset. 

0. Data Generation (DataGen.py)
line 306 : set Loader.addressOut into the path you want to store your processed data
line 307 : set Loader.addressOut_completeChar into the path you want to store your processed data
line 308 : set the abcnotation file location
line 310~312 : set all three path into where you want to store your processed data
line 198~200 : pick the mode you want from three cases, comment the other two
   three cases:
        datapreprocessing             : generate shifting tune dataset
        datapreprocessing_midi2largecharbase    : generate CharBase dataset
        datapreprocessing_midi2GenreClass    : generate seperate genre dataset
6. comment line 324
7. run the program
8. after generated dataset through step 1 to 7, change line 320 to your generated dataset path.
9. change line 322 to the path you want to store the trainable file. 
10. comment line 314 and uncommenet line 324 to generate trainable file. 


1. Data processing:
If your data is a stream of token represented by number 1~N separated by space, with a special number signal the end of
music. You can use process_midi2.py to create the training data with a fixed length window, shift amount and padding in
the end of music.

Please see the all_reel.dat as a reference of the output of this script

DEFAULT_EOF_SYM         : The end of music signal
SEGMENT_LEN             : window width
SEGMENT_SHIFT           : window shift
tr_output_fpath         : 80% of produced output will write into train file
te_output_fpath         : 20% will write into test file
data_arry = np.genfromtxt('./Data/raw/AllReel.txt',delimiter=' ')  : specifies the raw input data here
eof_sym = 96            : new token that signal the end of token in case you want to change

2. Training:
You can train the model by setting up config.py file and run either sequence_gan.py to train with a CNN discriminator or
sequence_GAN_RNN_DIS.py to train with a RNN discriminator. The parameters in config file are as follows:

EMB_DIM = 16                                     : embedding dimension
HIDDEN_DIM = 64                                  : hidden state dimension of lstm cell(generator)


PRE_EPOCH_NUM = 200                              : maximun pretrain epochs
SEED = 88
BATCH_SIZE = 64

dis_embedding_dim = EMB_DIM                                                     : Discriminator embedding dimension
dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]                      : CNN filter length
dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]  : CNN filter depth
dis_dropout_keep_prob = 0.75                    : Dropout keep rate for CNN discriminator(Not used in RNN)
dis_l2_reg_lambda = 0.2                         : L2 loss for CNN discriminator(Not used in case of RNN discriminator)
dis_batch_size = BATCH_SIZE
dis_train_iter = 5                              : how many time a discriminator should train in a D step
MAX_NUM_IN_EPOCH = 9700                         : number of examples generated for evaluating test loss
TOTAL_BATCH = 120                               : Maximum number of epoch for GAN training
SAMP_NUM = 50                                   : Number of MC sampling in rollout
## src file
positive_file = 'Data/midi2_len40.dat'          : file for training
eval_real_file = 'Data/midi2_len40_eval.dat'    : file for evaluating test loss
#tgt file
negative_file = 'Data/generator_sample.dat'     : location for generator to generate false example in D step
eval_file = 'Data/eval.dat'                     : location for generator to generate sample (not important)
generated_num = MAX_NUM_IN_EPOCH                : number of false example generated by generator in D step
START_TOKEN = 0                                 : a start input token to initiate generator, should be outside of vocab.
SEQ_LENGTH = 40                                 : training input sequence length
vocab_size = 95                                 : vocab size of training input


To train a 2 layer rnn as generator use Generator2 instead of Generator to create generator.

Note: this code is based on the [SequenceGAN](https://github.com/LantaoYu/SeqGAN).

3. Music Generation:
temp.py can be used to generate music for model trained with single layer generator.
temp2.py for 2 layer generator.

Some parameter in main need to specify:
fname = 'model12'                   : model name for model under './Model/'
ckpt_path = model_path +fname+'-5'  : '5' need to change to whatever epoch number you want to restore
dictionary_file = './Data/dict/TrainableMidi2-Table.txt'  : a dictionary file to map token back to abc format

The output of this will be abc format, and need to put a header such as:
X: 1
T: Uist Dance
R: reel
M: 4/4
L: 1/8
K: Cmaj

And then use a program such as abc2midi.exe to convert it to midi file.

 Enjoy!